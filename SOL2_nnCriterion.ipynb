{
 "metadata": {
  "language": "lua",
  "name": "",
  "signature": "sha256:cdb68cb9f2cdffd19fb4bba695f7805dffcb38c665c084cf43cf6838050c0904"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You just learned something about modules with :forward() :backward() and their weight/gradWeight members."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's have a look at what a criterion is, the two most often used criterions and the softmax layer.\n",
      "Read the documentation on https://github.com/torch/nn/blob/master/doc/criterion.md"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A criterion is a component that takes the output of a neural network and computes a loss function and the gradient of the loss. Similar to modules, a criterion defines a forward and backward function. A criterion usually doesn't have parameters so there will be no accumulation of gradients in a criterion.\n",
      "\n",
      "[output] forward(input, target)\n",
      "\n",
      "[gradInput] backward(input, target)\n",
      "\n",
      "The output of a criterion is the value of the objective function, you want to use it to measure your performance. The gradInput of a criterion is the gradient you want to apply to the neural network's outptut."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'nn';"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MSE Criterion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The MSE criterion is mostly used for regression problems. You use it to measure the mean squared error between the network output and the desired output.\n",
      "\n",
      "Let's assume we get ypred as output of a network that is learning a regression task (tries to predict 3 numbers from an output). ytrue is the true value.\n",
      "\n",
      "Note that all of the following is on a per-sample basis. This is what is most useful for SGD. For batch GD or minibatch GD you average the per-sample loss of many samples, and accumulate the gradients like discussed in the previous tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 3\n",
      "crit = nn.MSECriterion()\n",
      "ypred = torch.randn(N)\n",
      "ytrue = torch.randn(N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the mean square error with a for loop or through torch math operations see https://github.com/torch/torch7/blob/master/doc/maths.md"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "torch.pow(ypred-ytrue, 2):mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "3.5103563811209\t\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the MSE with the criterion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crit:forward(ypred, ytrue)\n",
      "print(crit.output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "3.5103563811209\t\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the derivative of the MSE with respect to the i'th component of the predicted output by hand (pen and paper). Use torch to calculate the gradient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(ypred - ytrue) * 2/N"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "-1.8308\n",
        " 1.1245\n",
        "-0.2534\n",
        "[torch.DoubleTensor of dimension 3]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the criterion to calculate the derivatives."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crit:backward(ypred, ytrue)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "-1.8308\n",
        " 1.1245\n",
        "-0.2534\n",
        "[torch.DoubleTensor of dimension 3]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NLLCriterion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The NLL criterion is the workhorse of NN classification training. In the lectures Prof. LeCun will go into detail about this criterion, why it is good for classification, it's link to KL divergence if its inputs are logsoftmax, and that you should calculate fprop & backprop through a softmax + NLL by hand.\n",
      "\n",
      "The NLLcriterion assumes it gets log-probabilities of each class. The target is the class (an integer)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crit = nn.ClassNLLCriterion()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs = torch.rand(5)\n",
      "probs:div(probs:sum())\n",
      "logprobs = torch.log(probs)\n",
      "label = 3\n",
      "print(logprobs)\n",
      "print(\"check normalization: \" .. torch.sum(torch.exp(logprobs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "-1.3954\n",
        "-1.0433\n",
        "-1.9329\n",
        "-2.0945\n",
        "-2.0241\n",
        "[torch.DoubleTensor of dimension 5]\n",
        "\n",
        "check normalization: 1\t\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the NLL loss value? How to calculate it by hand?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(crit:forward(logprobs, 3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "1.932949307073\t\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What if the correct class' probability is high (close to 1)? What if it is low (close to 0)? Does this make sense from a \"loss function\" perspective?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the gradient wrt the criterion's input."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(crit:backward(logprobs, label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        " 0\n",
        " 0\n",
        "-1\n",
        " 0\n",
        " 0\n",
        "[torch.DoubleTensor of dimension 5]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now do the same thing if the input is a batch (2-dimensional tensor) and the labels is a 1D-tensor of labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Todo now: explore the logsoftmax layer to see how you can get log probabilities out of a network. https://github.com/torch/nn/blob/master/doc/transfer.md#logsoftmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}